## DocumentRAG - sample environment file
## Copy these into your own ".env" (same folder as docker-compose.yml) if you use one.
##
## Recommended local LLM (no keys/tokens): Ollama
## If you run Ollama via docker-compose, keep OLLAMA_URL as-is.
## If you run Ollama on your host, set OLLAMA_URL=http://localhost:11434

# Local LLM configuration (Ollama)
OLLAMA_URL=http://ollama:11434
OLLAMA_MODEL=qwen2.5:14b

# Embeddings (reduce Docker memory usage; you can switch to all-mpnet-base-v2 for quality)
EMBEDDING_MODEL_NAME=all-MiniLM-L6-v2
EMBEDDING_BATCH_SIZE=16

# Chunking / ingestion safety limits (prevents backend OOM / exit 137 on large PDFs)
MAX_CHUNKS_PER_DOC=300
CHUNK_MAX_CHARS=1600
CHUNK_OVERLAP_CHARS=250

# OCR (for scanned PDFs). Enable if you need it.
ENABLE_OCR=0
OCR_LANG=eng+hin+pan
OCR_TIMEOUT_S=600
OCR_MAX_PAGES=0

# Optional: TGI model server (not required)
TGI_URL=

# Frontend -> Backend URL (when running frontend locally with Node)
# Use 127.0.0.1 to avoid Windows IPv6 localhost quirks
NEXT_PUBLIC_API_URL=http://127.0.0.1:8000


