# AI Codebase Assistant ‚Äî Production-Ready Code Generation

A standalone, production-ready web tool designed for senior engineers and architects. It uses Retrieval-Augmented Generation (RAG) to understand entire repositories and generate complete, production-ready code implementations.

## üöÄ Key Features

- **Codebase RAG**: Deep understanding of your local repository via background indexing and semantic search.
- **Production-Ready Code**: Generates full, copy-pasteable implementations without skeletons or TODOs.
- **Structured Output**: Strict JSON validation and auto-recovery for deterministic LLM responses.
- **Security-First**: Path traversal protection, file size limits, and sanitization of AI-generated content.
- **Service-Oriented Architecture**: Clear separation between UI, API, LLM orchestration, and vector storage.

## üèóÔ∏è Architecture

The project is built with a focus on maintainability, security, and scalability:

- **Frontend**: Next.js (TypeScript) with modular components (`TaskInput`, `ResultViewer`) and Tailwind CSS for a professional UI.
- **Backend**: FastAPI (Python) using a service-layer pattern (`CodeGenerationService`).
- **Vector Store**: In-memory vector store with persistence (ChromaDB) for high-speed local retrieval.
- **LLM Orchestration**: Centralized prompts and robust parsing logic in `llm_client.py` and `services.py`.
- **Configuration**: Environment-based configuration using `pydantic-settings`.

## üõ†Ô∏è Quick Start

### 1. Requirements
- [Docker](https://www.docker.com/) and [Docker Compose](https://docs.docker.com/compose/)
- [Ollama](https://ollama.com/) (recommended for local inference)

### 2. Setup
Clone the repository and run:

```bash
# Start the entire stack (Backend + Frontend + Ollama)
docker compose up --build
```

### 3. Usage
1. Open `http://localhost:3000/assistant` in your browser.
2. Enter a coding task (e.g., "Implement a new authentication middleware using JWT").
3. View the generated summary, assumptions, and affected files.
4. Copy or download the generated code.

## ‚öôÔ∏è Configuration

The application can be configured via `.env` file or environment variables:

| Variable | Default | Description |
|----------|---------|-------------|
| `OLLAMA_URL` | `http://localhost:11434` | URL for the local Ollama service |
| `OLLAMA_MODEL` | `qwen2.5:14b` | LLM model to use for generation |
| `MAX_FILE_SIZE_MB` | `10` | Maximum size for uploaded/generated files |
| `ALLOWED_ORIGINS` | `["http://localhost:3000"]` | CORS allowed origins |

## üß™ Testing

```bash
# Run frontend tests
cd frontend
npm test
```

## üîí Security

- **No Code Execution**: The assistant only generates code; it never executes it on your host machine.
- **Path Sanitization**: All file paths generated by the AI are sanitized to prevent directory traversal.
- **Input Validation**: Strict Pydantic schemas for all API requests and responses.

## üìù License

MIT
